{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import tree\n",
    "from scipy import interp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = \"../datasets/\"\n",
    "dataPath = \"./data3/\"\n",
    "resultsPath = \"./results3/\"\n",
    "\n",
    "\n",
    "#If data fodler does not exists\n",
    "if not os.path.exists(resultsPath):\n",
    "    try:\n",
    "        os.makedirs(resultsPath)\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsList = pd.read_csv(dataPath+\"results-list.csv\")\n",
    "serie = []\n",
    "for i in resultsList['results']:\n",
    "    serie.append(i.split(\"-\")[2].split(\".\")[0])\n",
    "#print(serie)\n",
    "\n",
    "serie = pd.Series(serie)\n",
    "\n",
    "resultsList['type'] = serie\n",
    "accuracy = []\n",
    "for i in resultsList['results']:\n",
    "    dfTemp = pd.read_csv(i)\n",
    "    dfMean = dfTemp.groupby(['sr']).mean().drop(['t'],axis=1)\n",
    "    dfMean['accuracy']= (dfMean['TP']+dfMean['TN']) / (dfMean['TP']+dfMean['TN']+dfMean['FN']+dfMean['FP'])\n",
    "    accuracy.append(dfMean['accuracy'].mean())\n",
    "resultsList['accuracy'] = pd.Series(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "configSet = []\n",
    "resultsList2 = resultsList[idx][(resultsList['type'] == \"classification\") | (resultsList['type'] == \"regression\")].where(pd.notnull(resultsList), None)\n",
    "for f in resultsList2.file.unique():\n",
    "    for t in resultsList2.type.unique():\n",
    "        for index, r in resultsList2.loc[(resultsList[\"file\"]==f) & (resultsList2[\"type\"]==t)].sort_values(\"accuracy\", ascending=False)[:10].iterrows():\n",
    "            configuration = {}\n",
    "            for i in r.keys():\n",
    "                if not i in [\"results\",\"type\",\"accuracy\",\"file\"]:\n",
    "                    if not (r[\"type\"] == \"regression\" and i==\"class_weight\"):\n",
    "                        configuration[i] = r[i]\n",
    "            configSet.append({\"file\":r[\"file\"],\"type\":r[\"type\"],\"configuration\":configuration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfConfig = pd.DataFrame(configSet)\n",
    "for filename in dfConfig.file.unique():\n",
    "    \n",
    "    d = pd.read_csv(datasetPath+filename+\".csv\")\n",
    "    d = d.sort_values(by=\"perf\")\n",
    "\n",
    "    for threshold in [d[\"perf\"].iloc[i * d.shape[0]//4] for i in range(1, 4)]:\n",
    "\n",
    "        d[\"label\"] = 0\n",
    "        d.loc[d[\"perf\"] > threshold, \"label\"] = 1\n",
    "\n",
    "        X = d.drop([\"perf\"],axis=1,errors=\"ignore\")\n",
    "        y_reg = d[\"perf\"]\n",
    "        y_classif = d[\"label\"]\n",
    "        \n",
    "        n = X.shape[1] - 1\n",
    "\n",
    "        for sr in [n,2*n,3*n]:\n",
    "            NSUBS=10\n",
    "            shuffle_split = StratifiedShuffleSplit(train_size=sr, n_splits=NSUBS)\n",
    "\n",
    "            classifier = tree.DecisionTreeClassifier(**dfConfig[(dfConfig['file'] == filename) & (dfConfig['type'] == \"classification\")].configuration.values[0])\n",
    "            tprs = []\n",
    "            aucs = []\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "            i = 0\n",
    "            for train, test in shuffle_split.split(X, y_classif):\n",
    "                probas_ = classifier.fit(X.drop([\"label\"],axis=1).iloc[train], y_classif.iloc[train]).predict_proba(X.drop([\"label\"],axis=1).iloc[test])\n",
    "                # Compute ROC curve and area the curve\n",
    "                fpr, tpr, thresholds = roc_curve(y_classif.iloc[test], probas_[:, 1])\n",
    "                #print(thresholds)\n",
    "                tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "                tprs[-1][0] = 0.0\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                aucs.append(roc_auc)\n",
    "                #plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            mean_tpr = np.mean(tprs, axis=0)\n",
    "            mean_tpr[-1] = 1.0\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            std_auc = np.std(aucs)\n",
    "            plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                     label=r'Mean ROC Classification (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                     lw=2, alpha=.8)\n",
    "\n",
    "            plt.rcParams[\"figure.figsize\"] = [15,9]\n",
    "\n",
    "            std_tpr = np.std(tprs, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            #plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "            plt.plot(mean_fpr, tprs_lower, '--', color='b',\n",
    "                     label=r'Mean ROC Classification STD',\n",
    "                     lw=1, alpha=.8)\n",
    "\n",
    "            plt.plot(mean_fpr, tprs_upper, '--', color='b',\n",
    "                     #label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                     lw=1, alpha=.8)\n",
    "\n",
    "            classifier = tree.DecisionTreeRegressor(**dfConfig[(dfConfig['file'] == filename) & (dfConfig['type'] == \"regression\")].configuration.values[0])\n",
    "            tprs = []\n",
    "            aucs = []\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "            i = 0\n",
    "            for train, test in shuffle_split.split(X, y_classif):\n",
    "                probas_ = classifier.fit(X.iloc[train], y_reg.iloc[train]).predict(X.iloc[test]) > threshold\n",
    "                # Compute ROC curve and area the curve\n",
    "                fpr, tpr, thresholds = roc_curve(y_reg.iloc[test] > threshold, probas_)\n",
    "                #print(thresholds)\n",
    "                tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "                tprs[-1][0] = 0.0\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                aucs.append(roc_auc)\n",
    "                #plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            mean_tpr = np.mean(tprs, axis=0)\n",
    "            mean_tpr[-1] = 1.0\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            std_auc = np.std(aucs)\n",
    "            plt.plot(mean_fpr, mean_tpr, color='r',\n",
    "                     label=r'Mean ROC Regression (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                     lw=2, alpha=.8)\n",
    "\n",
    "            plt.rcParams[\"figure.figsize\"] = [15,9]\n",
    "\n",
    "            std_tpr = np.std(tprs, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            #plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "            plt.plot(mean_fpr, tprs_lower, '--', color='r',\n",
    "                     label=r'Mean ROC Regression STD',\n",
    "                     lw=1, alpha=.8)\n",
    "\n",
    "            plt.plot(mean_fpr, tprs_upper, '--', color='r',\n",
    "                     #label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                     lw=1, alpha=.8)\n",
    "\n",
    "            plt.xlim([-0.05, 1.05])\n",
    "            plt.ylim([-0.05, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Threshold='+str(threshold)+\"/Sample=\"+str(sr))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(resultsPath+'ROC_'+filename+'_'+str(threshold)+'_'+str(sr)+'.png')\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
