{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params for Decision Tree\n",
    "#Default params\n",
    "classParamsDefault = {\n",
    "    \"criterion\":\"gini\",\n",
    "    \"splitter\":\"best\",\n",
    "    \"max_features\":None,\n",
    "    \"max_depth\":None,\n",
    "    \"min_samples_split\":2,\n",
    "    \"min_samples_leaf\":1,\n",
    "    \"min_weight_fraction_leaf\":0.,\n",
    "    \"max_leaf_nodes\":None,\n",
    "    \"class_weight\":None,\n",
    "    \"random_state\":None,\n",
    "    \"min_impurity_decrease\":1e-7,\n",
    "    \"presort\":False\n",
    "}\n",
    "regParamsDefault = {\n",
    "    \"criterion\":\"mse\",\n",
    "    \"splitter\":\"best\",\n",
    "    \"max_depth\":None,\n",
    "    \"min_samples_split\":2,\n",
    "    \"min_samples_leaf\":1,\n",
    "    \"min_weight_fraction_leaf\":0.,\n",
    "    \"max_features\":None,\n",
    "    \"random_state\":None,\n",
    "    \"max_leaf_nodes\":None,\n",
    "    \"min_impurity_decrease\":1e-7,\n",
    "    \"presort\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def newVersionFilename(path, filename):\n",
    "    # Get all the files in the {path} directory starting with {filename}\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f)) and f.startswith(filename+\"-\")]\n",
    "    files.sort(reverse=True)\n",
    "    # If no file yet\n",
    "    if len(files)==0:\n",
    "        return path+filename+\"-\"+str(1).zfill(4)\n",
    "    # Split the last one\n",
    "    splitted = files[0].split(\"-\")\n",
    "    # Get the last version\n",
    "    num = int(splitted[len(splitted)-2].split(\".\")[0])\n",
    "    # Return the full name with new version\n",
    "    return path+filename+\"-\"+str(num+1).zfill(4)\n",
    "\n",
    "\n",
    "def sensitivity(dataPath, classifParams, regParams):\n",
    "\n",
    "    #If data fodler does not exists\n",
    "    if not os.path.exists(dataPath):\n",
    "        try:\n",
    "            os.makedirs(dataPath)\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    d = pd.read_csv(\"dataset.csv\") # Open dataset\n",
    "\n",
    "    resClassification = {\"sr\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[]}\n",
    "    resRegression = {\"sr\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[],\"MSE\":[]}\n",
    "    \n",
    "    perf=\"value that led to label\"\n",
    "    label=\"label (0 if usable and 1 if not)\"\n",
    "    \n",
    "    srm = 100\n",
    "    srM = d.shape[0]\n",
    "    srs = 100\n",
    "    NSUBS = 10\n",
    "    \n",
    "    t=0.5\n",
    "    \n",
    "    #for sr in range(1,99):\n",
    "    for sr in range(srm+1,int(0.9*srM),srs):\n",
    "\n",
    "        shuffle_split = StratifiedShuffleSplit(train_size=sr, n_splits=NSUBS)\n",
    "\n",
    "        TN = TP = FN = FP = MSE = 0 # Counters for regression results\n",
    "\n",
    "        c = tree.DecisionTreeRegressor(**regParams)\n",
    "\n",
    "        for train_index, test_index in shuffle_split.split(d,d[label]):\n",
    "            c.fit(d.drop([perf,label],axis=1).iloc[train_index], d[perf].iloc[train_index])\n",
    "            pred = c.predict(d.drop([perf,label],axis=1).iloc[test_index])\n",
    "            #print(list(pred))\n",
    "            #print(list(clean.label.iloc[test_index]))\n",
    "            #print()\n",
    "            dfTest = pd.DataFrame()\n",
    "            dfTest[\"perf\"] = d[perf].iloc[test_index]\n",
    "            dfTest[\"pred\"] = pred\n",
    "            dfTest[\"label\"] = d[label].iloc[test_index]\n",
    "            dfTest[\"label_pred\"] = 0\n",
    "            dfTest.loc[dfTest[\"pred\"] >= t, \"label_pred\"] = 1\n",
    "\n",
    "            MSE = mse(dfTest[\"perf\"],dfTest[\"pred\"])\n",
    "\n",
    "            TN += dfTest[(dfTest.label == 0) & (dfTest.label_pred == 0)].shape[0]\n",
    "            TP += dfTest[(dfTest.label == 1) & (dfTest.label_pred == 1)].shape[0]\n",
    "            FN += dfTest[(dfTest.label == 1) & (dfTest.label_pred == 0)].shape[0]\n",
    "            FP += dfTest[(dfTest.label == 0) & (dfTest.label_pred == 1)].shape[0]\n",
    "\n",
    "        resRegression[\"sr\"].append(sr)\n",
    "        resRegression[\"MSE\"].append(MSE/NSUBS)\n",
    "        resRegression[\"TN\"].append(TN/NSUBS)\n",
    "        resRegression[\"TP\"].append(TP/NSUBS)\n",
    "        resRegression[\"FN\"].append(FN/NSUBS)\n",
    "        resRegression[\"FP\"].append(FP/NSUBS)\n",
    "\n",
    "\n",
    "        TN = TP = FN = FP = 0 # Counters for classification results\n",
    "\n",
    "        clean = d.drop([perf],axis=1,errors=\"ignore\")\n",
    "\n",
    "        c = tree.DecisionTreeClassifier(**classifParams)\n",
    "\n",
    "        try:\n",
    "\n",
    "            for train_index, test_index in shuffle_split.split(clean,clean[label]):\n",
    "                c.fit(clean.drop([label],axis=1).iloc[train_index], clean[label].iloc[train_index])\n",
    "                pred = c.predict(clean.drop([label],axis=1).iloc[test_index])\n",
    "                #print(list(pred))\n",
    "                #print(list(clean.label.iloc[test_index]))\n",
    "                #print()\n",
    "                dfTest = pd.DataFrame()\n",
    "                dfTest[\"label\"] = clean[label].iloc[test_index]\n",
    "                dfTest[\"pred\"] = pred\n",
    "\n",
    "                TN += dfTest[(dfTest.label == 0) & (dfTest.pred == 0)].shape[0]\n",
    "                TP += dfTest[(dfTest.label == 1) & (dfTest.pred == 1)].shape[0]\n",
    "                FN += dfTest[(dfTest.label == 1) & (dfTest.pred == 0)].shape[0]\n",
    "                FP += dfTest[(dfTest.label == 0) & (dfTest.pred == 1)].shape[0]\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "            break\n",
    "\n",
    "        resClassification[\"sr\"].append(sr)\n",
    "        resClassification[\"TN\"].append(TN/NSUBS)\n",
    "        resClassification[\"TP\"].append(TP/NSUBS)\n",
    "        resClassification[\"FN\"].append(FN/NSUBS)\n",
    "        resClassification[\"FP\"].append(FP/NSUBS)\n",
    "\n",
    "        \n",
    "    newFilename = newVersionFilename(dataPath,\"video\")\n",
    "    pd.DataFrame(resClassification).to_csv(newFilename+\"-classification.csv\", index=False)\n",
    "    pd.DataFrame(resRegression).to_csv(newFilename+\"-regression.csv\", index=False)\n",
    "    \n",
    "    classifParamsUsed = dict(classifParams)\n",
    "    classifParamsUsed['file']=\"dataset\"\n",
    "    classifParamsUsed['results']=newFilename+\"-classification.csv\"\n",
    "    \n",
    "    regParamsUsed = dict(regParams)\n",
    "    regParamsUsed['file']=\"dataset\"\n",
    "    regParamsUsed['results']=newFilename+\"-regression.csv\"\n",
    "\n",
    "    dfParamsUsed = pd.DataFrame.from_dict([classifParamsUsed,regParamsUsed])\n",
    "    \n",
    "    # If params list does not exists, create it\n",
    "    if not os.path.exists(dataPath+\"results-list.csv\"):\n",
    "        dfParamsUsed.to_csv(dataPath+\"results-list.csv\", index=False)\n",
    "    # If the list already exists, add the params used\n",
    "    else:\n",
    "        paramList = pd.read_csv(dataPath+\"results-list.csv\")\n",
    "        frames = [paramList, dfParamsUsed]\n",
    "        paramList = pd.concat(frames)\n",
    "        pd.DataFrame(paramList).to_csv(dataPath+\"results-list.csv\", index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sensitivity(\"./data/\",classParamsDefault,regParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifParams = dict(classParamsDefault)\n",
    "regParams = dict(regParamsDefault)\n",
    "\n",
    "for max_depth in [None,10,20,30,40,50,60,70,80,90]:\n",
    "#for max_depth in [None,20,40,60]:\n",
    "    #for max_leaf_nodes in [None,20,40,60]:\n",
    "    #for max_leaf_nodes in [10,30,50,70]:\n",
    "    for max_leaf_nodes in [10,20,30,40,50,60,70,80,90]:\n",
    "        for min_samples_leaf in [1,2,4,6,8,10]:\n",
    "\n",
    "            classifParams = dict(classParamsDefault)\n",
    "            regParams = dict(regParamsDefault)\n",
    "\n",
    "            classifParams['criterion']=\"entropy\"\n",
    "            classifParams['max_leaf_nodes'] = max_leaf_nodes\n",
    "            classifParams['min_samples_leaf']=min_samples_leaf\n",
    "            classifParams['max_depth']=max_depth\n",
    "\n",
    "            regParams['min_samples_leaf']=min_samples_leaf\n",
    "            regParams['max_leaf_nodes'] = max_leaf_nodes\n",
    "            regParams['max_depth'] = max_depth\n",
    "\n",
    "            sensitivity(\"./data/\",classifParams,regParams)\n",
    "            '''sensitivity(datasetPath = datasetPath, dataPath = dataPath, filename = filename, perf = perf, NBINS = NBINS,\n",
    "                NSUBS = NSUBS, srm = srm, srM = srM[k], srs = srs[k], classifParams=classifParams, regParams=regParams)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def newVersionFilename(path, filename):\n",
    "    # Get all the files in the {path} directory starting with {filename}\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f)) and f.startswith(filename+\"-\")]\n",
    "    files.sort(reverse=True)\n",
    "    # If no file yet\n",
    "    if len(files)==0:\n",
    "        return path+filename+\"-\"+str(1).zfill(4)\n",
    "    # Split the last one\n",
    "    splitted = files[0].split(\"-\")\n",
    "    # Get the last version\n",
    "    num = int(splitted[len(splitted)-2].split(\".\")[0])\n",
    "    # Return the full name with new version\n",
    "    return path+filename+\"-\"+str(num+1).zfill(4)\n",
    "\n",
    "\n",
    "def sensitivity2(dataPath, classifParams, regParams):\n",
    "\n",
    "    #If data fodler does not exists\n",
    "    if not os.path.exists(dataPath):\n",
    "        try:\n",
    "            os.makedirs(dataPath)\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    d = pd.read_csv(\"dataset.csv\") # Open dataset\n",
    "\n",
    "    resClassification = {\"sr\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[]}\n",
    "    resRegression = {\"sr\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[],\"MSE\":[]}\n",
    "    \n",
    "    perf=\"value that led to label\"\n",
    "    label=\"label (0 if usable and 1 if not)\"\n",
    "    \n",
    "    srm = 100\n",
    "    srM = d.shape[0]\n",
    "    srs = 100\n",
    "    NSUBS = 10\n",
    "    \n",
    "    n = d.shape[1]-2\n",
    "    \n",
    "    t=0.5\n",
    "    \n",
    "    #for sr in range(1,99):\n",
    "    #for sr in range(srm+1,int(0.9*srM),srs):\n",
    "    for sr in [n,n*2,n*3,n*4,n*5]:\n",
    "\n",
    "        shuffle_split = StratifiedShuffleSplit(train_size=sr, n_splits=NSUBS)\n",
    "\n",
    "        TN = TP = FN = FP = MSE = 0 # Counters for regression results\n",
    "\n",
    "        c = tree.DecisionTreeRegressor(**regParams)\n",
    "\n",
    "        for train_index, test_index in shuffle_split.split(d,d[label]):\n",
    "            c.fit(d.drop([perf,label],axis=1).iloc[train_index], d[perf].iloc[train_index])\n",
    "            pred = c.predict(d.drop([perf,label],axis=1).iloc[test_index])\n",
    "            #print(list(pred))\n",
    "            #print(list(clean.label.iloc[test_index]))\n",
    "            #print()\n",
    "            dfTest = pd.DataFrame()\n",
    "            dfTest[\"perf\"] = d[perf].iloc[test_index]\n",
    "            dfTest[\"pred\"] = pred\n",
    "            dfTest[\"label\"] = d[label].iloc[test_index]\n",
    "            dfTest[\"label_pred\"] = 0\n",
    "            dfTest.loc[dfTest[\"pred\"] >= t, \"label_pred\"] = 1\n",
    "\n",
    "            MSE = mse(dfTest[\"perf\"],dfTest[\"pred\"])\n",
    "\n",
    "            TN += dfTest[(dfTest.label == 0) & (dfTest.label_pred == 0)].shape[0]\n",
    "            TP += dfTest[(dfTest.label == 1) & (dfTest.label_pred == 1)].shape[0]\n",
    "            FN += dfTest[(dfTest.label == 1) & (dfTest.label_pred == 0)].shape[0]\n",
    "            FP += dfTest[(dfTest.label == 0) & (dfTest.label_pred == 1)].shape[0]\n",
    "\n",
    "        resRegression[\"sr\"].append(sr)\n",
    "        resRegression[\"MSE\"].append(MSE/NSUBS)\n",
    "        resRegression[\"TN\"].append(TN/NSUBS)\n",
    "        resRegression[\"TP\"].append(TP/NSUBS)\n",
    "        resRegression[\"FN\"].append(FN/NSUBS)\n",
    "        resRegression[\"FP\"].append(FP/NSUBS)\n",
    "\n",
    "\n",
    "        TN = TP = FN = FP = 0 # Counters for classification results\n",
    "\n",
    "        clean = d.drop([perf],axis=1,errors=\"ignore\")\n",
    "\n",
    "        c = tree.DecisionTreeClassifier(**classifParams)\n",
    "\n",
    "        try:\n",
    "\n",
    "            for train_index, test_index in shuffle_split.split(clean,clean[label]):\n",
    "                c.fit(clean.drop([label],axis=1).iloc[train_index], clean[label].iloc[train_index])\n",
    "                pred = c.predict(clean.drop([label],axis=1).iloc[test_index])\n",
    "                #print(list(pred))\n",
    "                #print(list(clean.label.iloc[test_index]))\n",
    "                #print()\n",
    "                dfTest = pd.DataFrame()\n",
    "                dfTest[\"label\"] = clean[label].iloc[test_index]\n",
    "                dfTest[\"pred\"] = pred\n",
    "\n",
    "                TN += dfTest[(dfTest.label == 0) & (dfTest.pred == 0)].shape[0]\n",
    "                TP += dfTest[(dfTest.label == 1) & (dfTest.pred == 1)].shape[0]\n",
    "                FN += dfTest[(dfTest.label == 1) & (dfTest.pred == 0)].shape[0]\n",
    "                FP += dfTest[(dfTest.label == 0) & (dfTest.pred == 1)].shape[0]\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "            break\n",
    "\n",
    "        resClassification[\"sr\"].append(sr)\n",
    "        resClassification[\"TN\"].append(TN/NSUBS)\n",
    "        resClassification[\"TP\"].append(TP/NSUBS)\n",
    "        resClassification[\"FN\"].append(FN/NSUBS)\n",
    "        resClassification[\"FP\"].append(FP/NSUBS)\n",
    "\n",
    "        \n",
    "    newFilename = newVersionFilename(dataPath,\"video\")\n",
    "    pd.DataFrame(resClassification).to_csv(newFilename+\"-classification.csv\", index=False)\n",
    "    pd.DataFrame(resRegression).to_csv(newFilename+\"-regression.csv\", index=False)\n",
    "    \n",
    "    classifParamsUsed = dict(classifParams)\n",
    "    classifParamsUsed['file']=\"dataset\"\n",
    "    classifParamsUsed['results']=newFilename+\"-classification.csv\"\n",
    "    \n",
    "    regParamsUsed = dict(regParams)\n",
    "    regParamsUsed['file']=\"dataset\"\n",
    "    regParamsUsed['results']=newFilename+\"-regression.csv\"\n",
    "\n",
    "    dfParamsUsed = pd.DataFrame.from_dict([classifParamsUsed,regParamsUsed])\n",
    "    \n",
    "    # If params list does not exists, create it\n",
    "    if not os.path.exists(dataPath+\"results-list.csv\"):\n",
    "        dfParamsUsed.to_csv(dataPath+\"results-list.csv\", index=False)\n",
    "    # If the list already exists, add the params used\n",
    "    else:\n",
    "        paramList = pd.read_csv(dataPath+\"results-list.csv\")\n",
    "        frames = [paramList, dfParamsUsed]\n",
    "        paramList = pd.concat(frames)\n",
    "        pd.DataFrame(paramList).to_csv(dataPath+\"results-list.csv\", index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifParams = dict(classParamsDefault)\n",
    "regParams = dict(regParamsDefault)\n",
    "\n",
    "#for max_depth in [None,10,20,30,40,50,60,70,80,90]:\n",
    "for max_depth in [None,10,20,30,40,50]:\n",
    "    #for max_leaf_nodes in [None,20,40,60]:\n",
    "    #for max_leaf_nodes in [10,30,50,70]:\n",
    "    for max_leaf_nodes in [10,20,30,40,50,60,70,80,90]:\n",
    "        for min_samples_leaf in [1,2,4,6,8,10]:\n",
    "\n",
    "            classifParams = dict(classParamsDefault)\n",
    "            regParams = dict(regParamsDefault)\n",
    "\n",
    "            classifParams['criterion']=\"entropy\"\n",
    "            classifParams['max_leaf_nodes'] = max_leaf_nodes\n",
    "            classifParams['min_samples_leaf']=min_samples_leaf\n",
    "            classifParams['max_depth']=max_depth\n",
    "\n",
    "            regParams['min_samples_leaf']=min_samples_leaf\n",
    "            regParams['max_leaf_nodes'] = max_leaf_nodes\n",
    "            regParams['max_depth'] = max_depth\n",
    "\n",
    "            sensitivity2(\"./data2/\",classifParams,regParams)\n",
    "            '''sensitivity(datasetPath = datasetPath, dataPath = dataPath, filename = filename, perf = perf, NBINS = NBINS,\n",
    "                NSUBS = NSUBS, srm = srm, srM = srM[k], srs = srs[k], classifParams=classifParams, regParams=regParams)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
